#+TITLE: Hardcaml implementation of MSM for the BLS12-377 - Notes

Reference: [[https://zprize.hardcaml.com/msm-overview][Hardcaml MSM implementation for zprize]]

- Tasked to building an FPGA design to multiply 2^26 points on the BLS12-377 elliptic curve by scalars from the associated 253 bit scalar field and add them all as fast as possible.
  - Multiply and accumulate operation of sorts. Probably highly parallelized with multiple compute elements and utilizing high-bandwidth memory (HBM) I presume.
- The challenge with computation of point addition and point multiplication is that both are very expensive operations in terms of compute and memory utilization.
  - I believe this is the drawback of performing these operations on general, non-domain-optimized or non-domain-specific compute.
- Their target platform was an Amazon f1.2xlarge instance which uses a *Xilinx UltraScale+ VU9P FPGA* with DDR memory banks (DDR4). This FPGA has PCIe lanes integrated as well as high-performance DSP cores.
- They mention that they have utilized the *aws-fpga Vitis flow* for implementation.
  - Will explore if HLS might be a viable route to implement for BN254.

** Top-level Pippenger Design

- The main idea of Pippenger's Algorithm is to reformulate the dot products into smaller dot products over windows, where each window represents a small contiguous set of bits from the scalar.

** Overview of the Architecture

- In the case of BLS12-377, the prime field and scalar field are 377 and 253 bit respectively.
- In their implementation, they have partitioned the work that the FPGA performs the bucket aggregation and the host performs the final bucket sum.
  - Will check if this partitioning is required. If the FPGA can perform the final bucket sum, we _might_ save some cycles in host-FPGA context switch.
- They claim the reason to partition it in this way is because when processing multiple MSMs, this partitioning allows them to mask out the latency of bucket sum by starting the bucket aggregation on the next MSM while computing bucket sum for the current MSM at host.
  - It makes sense to do so, I wonder if we can utilized some more LUTs on the FPGA for the final bucket sum and allow a pipelined buffer to manage the latency and see if that is viable.
- They have chosen B = 13 and W = 20 in their implementation, as this setup uses about 60% of the memory resources available.
- The time taken for bucket sum is around 1/10th the time taken for bucket aggregation.
  - Bucket sum isn't a very intensive operation from the looks of it, hence the host performing it makes sense. Still curious if there is anything to accelerate.
- They have discuss some ideas on improving the performance further in the future work section. (TODO tag future work section)

** FPGA Dataflow

- The high level block diagram showing the dataflow used in Hardcaml's MSM implementation -> [[https://zprize.hardcaml.com/images/msm-block-diagram.png][FPGA Dataflow block diagram]]
- Points are transformed into a custom representation and pre-loaded into DDR4 memory -> pre-processing data to create a custom structure to store, access and move the data when required to.
- They say this is done so that, at run-time only scalars are sent from the host to the FPGA via PCIe.
- A single fully-pipelined point added on the FPGA which adds points to buckets as directed by a Pippenger Controller until there is no more points left (in memory or the FIFO buffers (if any)).
  - Pippenger Controller must be using some sort of straight-forward arbitration to ensure all the points are fed to the adder, I will check if the controller arbitration might be a bottleneck. Or if we can improve the controller to see any considerable improvements.
  - Another thing I would like to check if we can unroll the adder in anyway possible to pipeline larger operations into micro operations and reduce latency that way.
- Once all points have been added into buckets, the FPGA streams back the result for the host to do the final bucket sum.
- They say this approach allows them to focus on implementing a very high performance added on the FPGA (seems like these addition operations are the dominating operation in the Pippenger's algorithm) and then leaving smaller tasks for the host to perform.
  - I will look for their benchmark results and see if they have done any sort of profiling.

** FPGA Bucket Sum

- In their implementation, the majority of the computation is performed by a *fully pipelined point added* with a high but static latency of above 200 cycles.
- They say, that if a coefficient needs to be naively added to a bucket that is currently in use by the point adder, they need to wait to do so until the addition operation is fully complete before trying again.
  - Seems like a FIFO of depth 2 or 3 can be added to the adder to facilitate issuing coefficients that need to be added to a bucket in between.
- They have discussed more about minimizing the impact of the ~200 cycle latency per operation in the page about the Pippenger Controller. (TODO tag the Pippenger Controller page)
- They seem to be utilizing a trick to reduce the memory usage *by transforming the scalar into signed digit representation for scalars for every bucket*.

** FPGA Point Adder

- The most expensive part of the pipelined point adder computation is *field multiplications*.
  - Will do literature survey to see if there are any novel methods to mitigate this compute expense with field addition and field multiplications.
- Their implementation is based on Barrett Reduction.
  - I will read more about reduction methods and see if trying a different reduction method can fetch us performance gains.
- To reduce the number of field multiplications, *they convert the points representation* from its original Weistrass curve form into a *Twisted Edwards curve* representation. This reduces the amount of field multiplication substantially.
  - I will look into more about these curve representations.
- They have gone one step further to reduce the field multiplication operations with some precomputation tricks in the adder implementation. (TODO tag the precomputation tricks page)
